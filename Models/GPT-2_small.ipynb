{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/valentin/Github/ai-multi-modal/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import GPT2Model, GPT2TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer_Fast:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "        self.model = GPT2Model.from_pretrained('gpt2')\n",
    "        self.tokens = None\n",
    "        self.tokens_ids = None\n",
    "        self.embeddings = None\n",
    "\n",
    "    def encode(self, text):\n",
    "        self.tokens = self.tokenizer.tokenize(text)\n",
    "        self.tokens_ids = self.tokenizer.convert_tokens_to_ids(self.tokens)\n",
    "\n",
    "        input_ids = torch.tensor(self.tokens_ids).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids)\n",
    "            self.embeddings = outputs.last_hidden_state\n",
    "        # print(self.embeddings)\n",
    "    \n",
    "    def decode(self, output_ids):\n",
    "        return self.tokenizer.decode(output_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT_2_small:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = Tokenizer_Fast()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tokenizer.model.to(self.device)\n",
    "\n",
    "    def forward(self, input_ids):  \n",
    "        None    \n",
    "    \n",
    "    def inference (self, text):\n",
    "        self.tokenizer.encode(text)\n",
    "        print(self.tokenizer.tokens)\n",
    "        print(self.tokenizer.tokens_ids)\n",
    "        print(self.tokenizer.embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT = GPT_2_small()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bon', 'j', 'our', ',', 'Ġje', 'Ġm', \"'\", 'app', 'elle', 'ĠJean']\n",
      "[20682, 73, 454, 11, 11223, 285, 6, 1324, 13485, 11320]\n",
      "tensor([[[-0.0204, -0.0216, -0.1747,  ..., -0.1526, -0.0710, -0.3193],\n",
      "         [-0.7794, -0.4617, -1.1806,  ...,  0.0930,  0.0920,  0.5608],\n",
      "         [-0.0570,  0.0280,  0.0119,  ...,  0.4866,  0.0564, -0.6508],\n",
      "         ...,\n",
      "         [-0.2427, -0.4578,  0.3239,  ..., -0.2223, -0.3328,  0.1226],\n",
      "         [-0.2271, -0.5400,  0.8330,  ..., -0.0763, -0.7190, -0.2155],\n",
      "         [ 0.1597, -0.1914, -0.2994,  ..., -0.0721, -0.3897, -0.0570]]])\n"
     ]
    }
   ],
   "source": [
    "text = 'Bonjour, je m\\'appelle Jean'\n",
    "\n",
    "GPT.inference(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there was no such thing as a man who was not a member of the Church of Jesus Christ of Latter-day Saints.\n",
      "\n",
      "The Church was founded by Joseph Smith, the Prophet of Mormonism, and his wife, Lucy Mack Smith. They were the first women to be ordained to the priesthood. The Church's first president, Joseph Fielding Smith Jr., was born in Nauvoo, Illinois, in 1844. He was the youngest of five children. His father\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = 'gpt2'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define a prompt\n",
    "prompt_text = \"Once upon a time,\"\n",
    "input_ids = tokenizer.encode(prompt_text, return_tensors='pt')\n",
    "\n",
    "# Generate text\n",
    "output = model.generate(input_ids, max_length=100, num_return_sequences=1, no_repeat_ngram_size=2, num_beams=5)\n",
    "\n",
    "# Decode and print the generated text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = 'gpt2'  # You can also use 'gpt2-medium', 'gpt2-large', 'gpt2-xl' for larger models\n",
    "model = GPT2Model.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
