{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/valentin/Github/ai-multi-modal/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import GPT2Model, GPT2TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer_Fast:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "        self.model = GPT2Model.from_pretrained('gpt2')\n",
    "        self.tokens = None\n",
    "        self.tokens_ids = None\n",
    "        self.embeddings = None\n",
    "\n",
    "    def encode(self, text):\n",
    "        self.tokens = self.tokenizer.tokenize(text)\n",
    "        self.tokens_ids = self.tokenizer.convert_tokens_to_ids(self.tokens)\n",
    "\n",
    "        input_ids = torch.tensor(self.tokens_ids).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids)\n",
    "            self.embeddings = outputs.last_hidden_state\n",
    "        # print(self.embeddings)\n",
    "    \n",
    "    def decode(self, output_ids):\n",
    "        return self.tokenizer.decode(output_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT_2_small:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = Tokenizer_Fast()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tokenizer.model.to(self.device)\n",
    "\n",
    "    def forward(self, input_ids):  \n",
    "        None    \n",
    "    \n",
    "    def inference (self, text):\n",
    "        return self.tokenizer.encode(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT = GPT_2_small()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 μs, sys: 0 ns, total: 4 μs\n",
      "Wall time: 6.91 μs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "text = \"Bonjour, je m'appelle Jean\"\n",
    "\n",
    "GPT.inference(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there was no such thing as a man who was not a member of the Church of Jesus Christ of Latter-day Saints.\n",
      "\n",
      "The Church was founded by Joseph Smith, the Prophet of Mormonism, and his wife, Lucy Mack Smith. They were the first women to be ordained to the priesthood. The Church's first president, Joseph Fielding Smith Jr., was born in Nauvoo, Illinois, in 1844. He was the youngest of five children. His father\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = 'gpt2'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define a prompt\n",
    "prompt_text = \"Once upon a time,\"\n",
    "input_ids = tokenizer.encode(prompt_text, return_tensors='pt')\n",
    "\n",
    "# Generate text\n",
    "output = model.generate(input_ids, max_length=100, num_return_sequences=1, no_repeat_ngram_size=2, num_beams=5)\n",
    "\n",
    "# Decode and print the generated text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = 'gpt2'  # You can also use 'gpt2-medium', 'gpt2-large', 'gpt2-xl' for larger models\n",
    "model = GPT2Model.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in ./venv/lib/python3.12/site-packages (0.7.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./venv/lib/python3.12/site-packages (from tiktoken) (2024.5.15)\n",
      "Requirement already satisfied: requests>=2.26.0 in ./venv/lib/python3.12/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20682, 73, 454, 11, 11223, 285, 6, 1324, 13485, 11320]\n",
      "Bonjour, je m'appelle Jean\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "test = enc.encode(\"Bonjour, je m'appelle Jean\")\n",
    "\n",
    "print(test)\n",
    "\n",
    "print(enc.decode(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 μs, sys: 0 ns, total: 4 μs\n",
      "Wall time: 10 μs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "test = enc.encode(\"Bonjour, je m'appelle Jean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 μs, sys: 1 μs, total: 3 μs\n",
      "Wall time: 4.53 μs\n",
      "Encoded input: [15496, 11, 703, 389, 345, 30]\n",
      "Decoded output: Hello, how are you?\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Encode a text string\n",
    "text = \"Hello, how are you?\"\n",
    "encoded_input = tokenizer.encode(text)\n",
    "print(\"Encoded input:\", encoded_input)\n",
    "\n",
    "# Decode the token IDs back to text\n",
    "decoded_output = tokenizer.decode(encoded_input)\n",
    "print(\"Decoded output:\", decoded_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 μs, sys: 1 μs, total: 4 μs\n",
      "Wall time: 7.15 μs\n",
      "Encoded input: [15496, 11, 703, 389, 345, 30]\n",
      "Decoded output: Hello, how are you?\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "import tiktoken\n",
    "\n",
    "# Load the tokenizer for the desired model\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Encode a text string\n",
    "text = \"Hello, how are you?\"\n",
    "encoded_input = enc.encode(text)\n",
    "print(\"Encoded input:\", encoded_input)\n",
    "\n",
    "# Decode the token IDs back to text\n",
    "decoded_output = enc.decode(encoded_input)\n",
    "print(\"Decoded output:\", decoded_output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
